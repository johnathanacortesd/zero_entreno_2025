{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Tono y tema sin datos de entrenamiento"
      ],
      "metadata": {
        "id": "8T0NTLANOshM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tono"
      ],
      "metadata": {
        "id": "HsLbP1EDcX38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Instalar librerías\n",
        "# Instalar librerías necesarias de forma silenciosa\n",
        "!pip install -qq sentence-transformers pandas openpyxl tqdm emoji\n",
        "\n",
        "# Importar librerías\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import emoji\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import unicodedata\n",
        "from google.colab import userdata # Import userdata to access secrets if needed for other models\n",
        "\n",
        "# Verificar y configurar el uso de GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "    print(\"Using GPU for computation.\")\n",
        "else:\n",
        "    device = 'cpu'\n",
        "    print(\"Using CPU for computation.\")\n",
        "\n",
        "# Definir las categorías de sentimiento y sus palabras clave\n",
        "# Estas palabras clave son descriptivas y ayudan al modelo a entender el 'centro' de cada sentimiento.\n",
        "# Sin embargo, el modelo ya está pre-entrenado para entender el sentimiento directamente.\n",
        "sentiment_keywords = {\n",
        "    \"Positivo\": [\n",
        "        \"excelente\", \"bueno\", \"genial\", \"positivo\", \"fantástico\", \"maravilloso\",\n",
        "        \"feliz\", \"contento\", \"agradable\", \"éxito\", \"perfecto\", \"gran\", \"magnífico\",\n",
        "        \"óptimo\", \"mejor\", \"acuerdo\", \"aprobación\", \"felicitaciones\", \"elogio\"\n",
        "    ],\n",
        "    \"Neutro\": [\n",
        "        \"neutral\", \"información\", \"hecho\", \"dato\", \"reporte\", \"observación\",\n",
        "        \"análisis\", \"declaración\", \"noticia\", \"comunicado\", \"menciona\", \"describe\",\n",
        "        \"sin emoción\", \"objetivo\", \"estado\", \"situación\", \"acerca de\", \"sobre\"\n",
        "    ],\n",
        "    \"Negativo\": [\n",
        "        \"malo\", \"terrible\", \"negativo\", \"problema\", \"crisis\", \"difícil\", \"triste\",\n",
        "        \"decepcionante\", \"preocupante\", \"fallo\", \"error\", \"desastre\", \"crítica\",\n",
        "        \"queja\", \"inconformidad\", \"pésimo\", \"peor\", \"desacuerdo\", \"rechazo\", \"malestar\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# No verificamos palabras clave duplicadas para sentimiento ya que las superposiciones son más comunes\n",
        "# y el modelo se basa más en el significado contextual.\n",
        "\n",
        "# Cargar textos a clasificar\n",
        "from google.colab import files\n",
        "print(\"Sube el archivo con los textos a clasificar:\")\n",
        "uploaded_texts = files.upload()\n",
        "\n",
        "texts_file_name = list(uploaded_texts.keys())[0]\n",
        "texts_df = pd.read_excel(texts_file_name)\n",
        "\n",
        "if 'resumen' not in texts_df.columns:\n",
        "    raise ValueError(\"El archivo a clasificar debe contener una columna 'resumen'.\")\n",
        "\n",
        "# Cargar el modelo de Sentence-BERT específico para análisis de sentimiento en español\n",
        "# 'pysentimiento/robertuito-sentiment-analysis' es un modelo robusto y público para español.\n",
        "# Este modelo NO requiere token de Hugging Face.\n",
        "print(f\"Loading sentiment analysis model to {device}...\")\n",
        "sentiment_model = SentenceTransformer('pysentimiento/robertuito-sentiment-analysis', device=device)\n",
        "print(\"Sentiment analysis model loaded successfully.\")\n",
        "\n",
        "# Crear embeddings de las categorías de sentimiento\n",
        "# Aunque el modelo ya está pre-entrenado, usar estos embeddings como 'prototipos'\n",
        "# ayuda a guiar la clasificación por similitud cosenoidal.\n",
        "sentiment_embeddings = {}\n",
        "for sentiment, keywords in sentiment_keywords.items():\n",
        "    embeddings = sentiment_model.encode(keywords, convert_to_tensor=True)\n",
        "    sentiment_embedding = embeddings.mean(dim=0)\n",
        "    sentiment_embeddings[sentiment] = sentiment_embedding.to(device)\n",
        "\n",
        "# Apilar todos los embeddings de sentimiento en un solo tensor\n",
        "sentiment_embeddings_tensor = torch.stack(list(sentiment_embeddings.values()))\n",
        "sentiment_names = list(sentiment_embeddings.keys())\n",
        "\n",
        "# Función para normalizar el texto (la misma que antes)\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "    return text\n",
        "\n",
        "# Función para preprocesar el texto (la misma que antes)\n",
        "def preprocess_text(text):\n",
        "    text = normalize_text(text)\n",
        "    # Eliminar URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Eliminar caracteres especiales (excepto hashtags si fueran relevantes, pero para sentimiento no suelen serlo)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # Simplificado para sentimiento\n",
        "    return text\n",
        "\n",
        "# Función de clasificación de sentimiento\n",
        "# Ahora usamos un enfoque de 'coincidencia estricta' para palabras clave de sentimiento\n",
        "# y luego la similitud del modelo para el resto.\n",
        "def classify_sentiment(text, model, sentiment_embeddings_tensor, sentiment_names, threshold=0.6, min_score_diff=0.1):\n",
        "    text_preprocessed = preprocess_text(text)\n",
        "\n",
        "    # 1. Búsqueda de coincidencia exacta de palabras clave de sentimiento (alta confianza)\n",
        "    # Esta es una heurística fuerte para asegurar que \"excelente\" siempre sea Positivo.\n",
        "    for sentiment, keywords in sentiment_keywords.items():\n",
        "        for kw in keywords:\n",
        "            if normalize_text(kw) in text_preprocessed.split(): # Split para asegurar palabra completa\n",
        "                return sentiment, 1.0\n",
        "\n",
        "    # 2. Clasificación por similitud con embeddings de categorías (si no hay coincidencia exacta)\n",
        "    text_embedding = model.encode(text_preprocessed, convert_to_tensor=True).to(device)\n",
        "    cos_similarities = util.cos_sim(text_embedding, sentiment_embeddings_tensor)[0]\n",
        "\n",
        "    top_idx = torch.argmax(cos_similarities).item()\n",
        "    top_sentiment = sentiment_names[top_idx]\n",
        "    top_score = cos_similarities[top_idx].item()\n",
        "\n",
        "    # Evaluar la diferencia con la segunda mejor puntuación para mayor precisión\n",
        "    sorted_scores, _ = torch.sort(cos_similarities, descending=True)\n",
        "    if len(sorted_scores) > 1:\n",
        "        second_score = sorted_scores[1].item()\n",
        "        score_diff = top_score - second_score\n",
        "    else:\n",
        "        score_diff = top_score # Si solo hay una categoría, la diferencia es la propia puntuación\n",
        "\n",
        "    # Aplicar umbrales para decidir la confianza o si es \"Neutro\" por ambigüedad\n",
        "    if top_score >= threshold and score_diff >= min_score_diff:\n",
        "        return top_sentiment, top_score\n",
        "    else:\n",
        "        # Si la confianza no es alta o la diferencia es baja,\n",
        "        # puede indicar que el texto es más neutro o ambiguo.\n",
        "        # Podríamos forzar a \"Neutro\" o dejar la categoría de mayor similitud\n",
        "        # con una confianza menor. Optamos por dejar la de mayor similitud.\n",
        "        return top_sentiment, top_score\n",
        "\n",
        "\n",
        "# Clasificar textos y almacenar resultados\n",
        "texts = texts_df['resumen'].fillna(\"\").tolist()\n",
        "results = []\n",
        "\n",
        "# Definir umbrales de confianza para el análisis de sentimiento\n",
        "sentiment_threshold = 0.65  # Umbral de confianza para la clasificación de sentimiento\n",
        "sentiment_min_score_diff = 0.15 # Diferencia mínima entre top 1 y top 2 para clasificación clara\n",
        "\n",
        "# Procesamiento por lotes\n",
        "batch_size = 32 # Ajusta este valor según la memoria de la GPU\n",
        "num_batches = (len(texts) + batch_size - 1) // batch_size\n",
        "\n",
        "for i in tqdm(range(num_batches), desc=\"Clasificando sentimiento por lotes\"):\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = min((i + 1) * batch_size, len(texts))\n",
        "    batch_texts = texts[start_idx:end_idx]\n",
        "\n",
        "    # Preprocesar textos del lote (directamente aquí para evitar re-preprocesar en la función)\n",
        "    batch_preprocessed_texts = [preprocess_text(text) for text in batch_texts]\n",
        "\n",
        "    # Codificar todos los textos del lote a la vez para eficiencia en GPU\n",
        "    # Nota: No pasamos el token aquí, ya que 'pysentimiento/robertuito-sentiment-analysis' es público\n",
        "    batch_text_embeddings = sentiment_model.encode(batch_preprocessed_texts, convert_to_tensor=True).to(device)\n",
        "\n",
        "    for j, original_text in enumerate(batch_texts):\n",
        "        # Obtener el embedding pre-calculado para el texto actual del lote\n",
        "        current_text_embedding = batch_text_embeddings[j]\n",
        "        current_text_preprocessed = batch_preprocessed_texts[j]\n",
        "\n",
        "        # 1. Coincidencia exacta de palabras clave de sentimiento (prioridad alta)\n",
        "        found_exact_match = False\n",
        "        for sentiment, keywords in sentiment_keywords.items():\n",
        "            for kw in keywords:\n",
        "                # Usar .split() para asegurar que 'bueno' en 'muy bueno' no coincida con 'buen'\n",
        "                if normalize_text(kw) in current_text_preprocessed.split():\n",
        "                    results.append({\n",
        "                        \"texto\": original_text,\n",
        "                        \"sentimiento\": sentiment,\n",
        "                        \"confianza\": 1.0 # Confianza máxima si hay coincidencia exacta\n",
        "                    })\n",
        "                    found_exact_match = True\n",
        "                    break\n",
        "            if found_exact_match:\n",
        "                break\n",
        "\n",
        "        if found_exact_match:\n",
        "            continue # Si ya clasificamos por coincidencia exacta, pasamos al siguiente texto\n",
        "\n",
        "        # 2. Clasificación por similitud cosenoidal (para el resto de textos)\n",
        "        cos_similarities = util.cos_sim(current_text_embedding, sentiment_embeddings_tensor)[0]\n",
        "\n",
        "        top_idx = torch.argmax(cos_similarities).item()\n",
        "        top_sentiment = sentiment_names[top_idx]\n",
        "        top_score = cos_similarities[top_idx].item()\n",
        "\n",
        "        # Obtener la segunda mayor similitud para evaluar la diferencia\n",
        "        sorted_scores, _ = torch.sort(cos_similarities, descending=True)\n",
        "        if len(sorted_scores) > 1:\n",
        "            second_score = sorted_scores[1].item()\n",
        "            score_diff = top_score - second_score\n",
        "        else:\n",
        "            score_diff = top_score # Si solo hay una categoría, la diferencia es la puntuación misma\n",
        "\n",
        "        # Decidir la categoría basada en el umbral y la diferencia de puntuaciones\n",
        "        if top_score >= sentiment_threshold and score_diff >= sentiment_min_score_diff:\n",
        "            final_sentiment = top_sentiment\n",
        "            final_confidence = top_score\n",
        "        else:\n",
        "            # Si la confianza no es lo suficientemente alta o hay mucha ambigüedad,\n",
        "            # lo asignamos como Neutro para ser más conservadores, o la de mayor similitud\n",
        "            # con su score.\n",
        "            # Aquí, priorizamos la categoría de mayor similitud si no cumple los umbrales\n",
        "            # de \"alta confianza\" pero aún así necesitamos una clasificación.\n",
        "            final_sentiment = top_sentiment # O podrías forzar a \"Neutro\" aquí si prefieres.\n",
        "            final_confidence = top_score\n",
        "\n",
        "        results.append({\n",
        "            \"texto\": original_text,\n",
        "            \"sentimiento\": final_sentiment,\n",
        "            \"confianza\": final_confidence\n",
        "        })\n",
        "\n",
        "# Exportar resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_excel(\"sentiment_classification_results.xlsx\", index=False)\n",
        "print(\"Clasificación de sentimiento completada. Descarga el archivo:\")\n",
        "files.download(\"sentiment_classification_results.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "uSYKtZU1ceeK",
        "outputId": "dcf90983-7625-4d73-ab34-5e7e56319701",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tema"
      ],
      "metadata": {
        "id": "VJY1yHxSdAwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Tema\n",
        "# Instalar librerías necesarias de forma silenciosa\n",
        "!pip install -qq sentence-transformers pandas openpyxl tqdm emoji\n",
        "\n",
        "# Importar librerías\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import emoji\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import unicodedata\n",
        "from google.colab import userdata # Import userdata to access secrets (still good practice for other needs)\n",
        "\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    if hf_token is None:\n",
        "        print(\"HF_TOKEN secret not found. This is okay for the chosen public model.\")\n",
        "    else:\n",
        "        print(\"Hugging Face token accessed from Colab Secrets (not strictly needed for this model).\")\n",
        "except Exception as e:\n",
        "    print(f\"Error accessing HF_TOKEN secret: {e}. This is okay for the chosen public model.\")\n",
        "    hf_token = None\n",
        "\n",
        "\n",
        "# Verificar uso de GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "    print(\"Using GPU for computation.\")\n",
        "else:\n",
        "    device = 'cpu'\n",
        "    print(\"Using CPU for computation.\")\n",
        "\n",
        "# Definir 10 temas muy genéricos para análisis de contenido\n",
        "predefined_keywords = {\n",
        "    \"Noticias y Eventos Actuales\": [\n",
        "        \"noticia\", \"actualidad\", \"evento\", \"suceso\", \"última hora\", \"acontecimiento\",\n",
        "        \"incidente\", \"desarrollo\", \"informe\", \"periodismo\", \"reportaje\", \"cobertura\",\n",
        "        \"reciente\", \"hoy\", \"ayer\"\n",
        "    ],\n",
        "    \"Opiniones y Debates\": [\n",
        "        \"opinión\", \"debate\", \"discusión\", \"punto de vista\", \"perspectiva\", \"argumento\",\n",
        "        \"controversia\", \"polémica\", \"crítica\", \"elogio\", \"comentario\", \"sentimiento\",\n",
        "        \"postura\", \"análisis\", \"reflexión\"\n",
        "    ],\n",
        "    \"Productos y Servicios\": [\n",
        "        \"producto\", \"servicio\", \"marca\", \"empresa\", \"oferta\", \"lanzamiento\",\n",
        "        \"característica\", \"función\", \"precio\", \"disponibilidad\", \"calidad\",\n",
        "        \"experiencia\", \"uso\", \"beneficio\", \"solución\"\n",
        "    ],\n",
        "    \"Economía y Negocios\": [\n",
        "        \"economía\", \"negocio\", \"mercado\", \"inversión\", \"finanzas\", \"empresa\",\n",
        "        \"crecimiento\", \"crisis económica\", \"sector\", \"comercio\", \"desempleo\",\n",
        "        \"tendencia económica\", \"capital\", \"ingresos\", \"gastos\"\n",
        "    ],\n",
        "    \"Política y Gobierno\": [\n",
        "        \"política\", \"gobierno\", \"ley\", \"regulación\", \"decisión\", \"elecciones\",\n",
        "        \"partido\", \"líder\", \"democracia\", \"justicia\", \"reforma\", \"público\",\n",
        "        \"estado\", \"norma\", \"oficial\"\n",
        "    ],\n",
        "    \"Sociedad y Cultura\": [\n",
        "        \"sociedad\", \"cultura\", \"comunidad\", \"educación\", \"salud\", \"arte\",\n",
        "        \"historia\", \"valores\", \"tendencia social\", \"grupo\", \"tradición\",\n",
        "        \"identidad\", \"cambio social\", \"bienestar\", \"equidad\"\n",
        "    ],\n",
        "    \"Tecnología e Innovación\": [\n",
        "        \"tecnología\", \"innovación\", \"digital\", \"internet\", \"software\", \"hardware\",\n",
        "        \"futuro\", \"desarrollo\", \"descubrimiento\", \"aplicación\", \"sistema\",\n",
        "        \"robótica\", \"inteligencia artificial\", \"conectividad\", \"ciberseguridad\"\n",
        "    ],\n",
        "    \"Medio Ambiente y Sostenibilidad\": [\n",
        "        \"medio ambiente\", \"sostenibilidad\", \"cambio climático\", \"ecología\",\n",
        "        \"contaminación\", \"recursos naturales\", \"biodiversidad\", \"energía\",\n",
        "        \"conservación\", \"reciclaje\", \"impacto ambiental\", \"verde\", \"planeta\",\n",
        "        \"residuos\"\n",
        "    ],\n",
        "    \"Salud y Bienestar\": [\n",
        "        \"salud\", \"bienestar\", \"enfermedad\", \"tratamiento\", \"medicina\", \"hospital\",\n",
        "        \"cuidado\", \"prevención\", \"alimentación\", \"ejercicio\", \"salud mental\",\n",
        "        \"diagnóstico\", \"síntoma\", \"recuperación\", \"terapia\"\n",
        "    ],\n",
        "    \"Deportes y Entretenimiento\": [\n",
        "        \"deporte\", \"entretenimiento\", \"película\", \"música\", \"serie\", \"juego\",\n",
        "        \"artista\", \"celebridad\", \"evento deportivo\", \"cultura pop\", \"diversión\",\n",
        "        \"espectáculo\", \"campeonato\", \"afición\", \"creativo\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Verificar que no haya palabras clave duplicadas entre categorías\n",
        "keyword_to_topics = defaultdict(list)\n",
        "for topic, keywords in predefined_keywords.items():\n",
        "    for kw in keywords:\n",
        "        keyword_to_topics[kw.lower()].append(topic)\n",
        "\n",
        "# Encontrar palabras clave que se repiten en múltiples categorías\n",
        "duplicate_keywords = {kw: topics for kw, topics in keyword_to_topics.items() if len(topics) > 1}\n",
        "\n",
        "if duplicate_keywords:\n",
        "    print(\"Palabras clave duplicadas encontradas:\")\n",
        "    for kw, topics in duplicate_keywords.items():\n",
        "        print(f\"'{kw}' se encuentra en las categorías: {topics}\")\n",
        "else:\n",
        "    print(\"No se encontraron palabras clave duplicadas entre categorías.\")\n",
        "\n",
        "# Cargar textos a clasificar\n",
        "from google.colab import files\n",
        "print(\"Sube el archivo con los textos a clasificar:\")\n",
        "uploaded_texts = files.upload()\n",
        "\n",
        "texts_file_name = list(uploaded_texts.keys())[0]\n",
        "texts_df = pd.read_excel(texts_file_name)\n",
        "\n",
        "if 'resumen' not in texts_df.columns:\n",
        "    raise ValueError(\"El archivo a clasificar debe contener una columna 'resumen'.\")\n",
        "\n",
        "# Cargar el modelo de Sentence-BERT (AHORA USANDO UN MODELO PÚBLICO Y ACCESIBLE)\n",
        "embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2', device=device) # Removed 'token=hf_token'\n",
        "\n",
        "# Crear embeddings de temas\n",
        "topic_embeddings = {}\n",
        "for topic, keywords in predefined_keywords.items():\n",
        "    if keywords:\n",
        "        embeddings = embedding_model.encode(keywords, convert_to_tensor=True)\n",
        "        topic_embedding = embeddings.mean(dim=0)\n",
        "    else:\n",
        "        topic_embedding = torch.zeros(embedding_model.get_sentence_embedding_dimension())\n",
        "    topic_embeddings[topic] = topic_embedding.to(device)\n",
        "\n",
        "# Apilar todos los embeddings de temas en un solo tensor\n",
        "topic_embeddings_tensor = torch.stack(list(topic_embeddings.values()))\n",
        "topic_names = list(topic_embeddings.keys())\n",
        "\n",
        "# Función para normalizar el texto\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "    return text\n",
        "\n",
        "# Función para preprocesar el texto\n",
        "def preprocess_text(text):\n",
        "    text = normalize_text(text)\n",
        "    # Eliminar URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Eliminar caracteres especiales excepto hashtags\n",
        "    text = re.sub(r'[^\\w#\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Función para verificar coincidencia exacta de palabras clave\n",
        "def exact_match_all(text, predefined_keywords):\n",
        "    matched_categories = []\n",
        "    # Tokenizar el texto para incluir palabras y hashtags\n",
        "    tokens = re.findall(r'#\\w+|\\w+', text)\n",
        "    # Normalizar tokens\n",
        "    normalized_tokens = [normalize_text(tok) for tok in tokens]\n",
        "\n",
        "    for topic, keywords in predefined_keywords.items():\n",
        "        for kw in keywords:\n",
        "            kw_normalized = normalize_text(kw)\n",
        "            if kw_normalized in normalized_tokens:\n",
        "                matched_categories.append(topic)\n",
        "                break\n",
        "    return matched_categories\n",
        "\n",
        "# Clasificar textos\n",
        "texts = texts_df['resumen'].fillna(\"\").tolist()\n",
        "results = []\n",
        "\n",
        "# Definir umbral de confianza y diferencia mínima para clasificación\n",
        "threshold = 0.68  # Mantener un umbral alto para precisión\n",
        "min_score_diff = 0.15 # Mantener una buena diferencia para desempate claro\n",
        "\n",
        "# Procesamiento por lotes\n",
        "batch_size = 128 # Ajusta este valor según la memoria de la GPU\n",
        "num_batches = (len(texts) + batch_size - 1) // batch_size\n",
        "\n",
        "for i in tqdm(range(num_batches), desc=\"Clasificando textos por lotes\"):\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = min((i + 1) * batch_size, len(texts))\n",
        "    batch_texts = texts[start_idx:end_idx]\n",
        "\n",
        "    batch_preprocessed_texts = [preprocess_text(text) for text in batch_texts]\n",
        "\n",
        "    # Codificar todos los textos del lote a la vez\n",
        "    batch_embeddings = embedding_model.encode(batch_preprocessed_texts, convert_to_tensor=True).to(device)\n",
        "\n",
        "    for j, original_text in enumerate(batch_texts):\n",
        "        text_preprocessed = batch_preprocessed_texts[j]\n",
        "        text_embedding = batch_embeddings[j]\n",
        "\n",
        "        # Buscar coincidencias de palabras clave primero\n",
        "        matched_categories = exact_match_all(text_preprocessed, predefined_keywords)\n",
        "\n",
        "        if matched_categories:\n",
        "            from collections import Counter\n",
        "            category_counts = Counter(matched_categories)\n",
        "            top_category = category_counts.most_common(1)[0][0]\n",
        "            results.append({\n",
        "                \"texto\": original_text,\n",
        "                \"categoria\": top_category,\n",
        "                \"confianza\": 1.0  # Confianza alta si hay coincidencia exacta\n",
        "            })\n",
        "        else:\n",
        "            # Calcular similitudes con todos los temas\n",
        "            cos_similarities = util.cos_sim(text_embedding, topic_embeddings_tensor)[0]\n",
        "\n",
        "            # Obtener el índice del tema con mayor similitud\n",
        "            top_idx = torch.argmax(cos_similarities).item()\n",
        "            top_topic = topic_names[top_idx]\n",
        "            top_score = cos_similarities[top_idx].item()\n",
        "\n",
        "            # Obtener la segunda mayor similitud para evaluar la diferencia\n",
        "            sorted_scores, sorted_indices = torch.sort(cos_similarities, descending=True)\n",
        "            if len(sorted_scores) > 1:\n",
        "                second_score = sorted_scores[1].item()\n",
        "                score_diff = top_score - second_score\n",
        "            else:\n",
        "                score_diff = top_score # Si solo hay un tema, la diferencia es la puntuación misma\n",
        "\n",
        "            # Decidir la categoría basada en el umbral y la diferencia de puntuaciones\n",
        "            if top_score >= threshold and score_diff >= min_score_diff:\n",
        "                categoria = top_topic\n",
        "                confianza = top_score\n",
        "            else:\n",
        "                # Si no cumple los criterios de confianza, se asigna a la categoría de mayor similitud\n",
        "                # pero la confianza reflejará que la correspondencia no fue muy fuerte.\n",
        "                categoria = top_topic\n",
        "                confianza = top_score\n",
        "\n",
        "            results.append({\n",
        "                \"texto\": original_text,\n",
        "                \"categoria\": categoria,\n",
        "                \"confianza\": confianza\n",
        "            })\n",
        "\n",
        "# Exportar resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_excel(\"classification_results.xlsx\", index=False)\n",
        "print(\"Clasificación completada. Descarga el archivo:\")\n",
        "files.download(\"classification_results.xlsx\")"
      ],
      "metadata": {
        "id": "oqWkCtILdBwF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "cellView": "form",
        "outputId": "683c91b1-e950-4b24-b47b-8549a9300fa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "l-dQLgnu4ZkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tono y tema con datos de entrenamiento"
      ],
      "metadata": {
        "id": "5Iy1p9yeDOF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tono"
      ],
      "metadata": {
        "id": "aNsggWKA3CPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Instalación\n",
        "!pip install -qq sentiment-analysis-spanish scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CqgdiVb3D9Q",
        "outputId": "af56abfa-2cc6-41d4-b9d6-9a84e573cef9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Subir excel de entrenamiento\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import metrics\n",
        "from google.colab import files\n",
        "\n",
        "def cargar_y_preprocesar_datos(archivo):\n",
        "    df = pd.read_excel(archivo)\n",
        "\n",
        "    # Mapear etiquetas de texto a valores numéricos\n",
        "    label_mapping = {'Positivo': 1, 'Neutro': 0, 'Negativo': -1}\n",
        "    df['tono_numerico'] = df['tono'].map(label_mapping)\n",
        "\n",
        "    # Filtrar filas inválidas\n",
        "    df = df[df['resumen'].notna() & df['resumen'].apply(lambda x: isinstance(x, str))]\n",
        "    df = df[df['tono_numerico'].notna()]\n",
        "\n",
        "    return df\n",
        "\n",
        "def entrenar_modelo(df):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['resumen'], df['tono_numerico'], test_size=0.2, random_state=42)\n",
        "\n",
        "    model = make_pipeline(CountVectorizer(min_df=1), LogisticRegression())\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluar el modelo\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = metrics.accuracy_score(y_test, predictions)\n",
        "    print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def analizar_nuevo_dataset(model, archivo):\n",
        "    df_new = pd.read_excel(archivo)\n",
        "\n",
        "    # Procesar fila por fila\n",
        "    resultados = []\n",
        "    for idx, row in df_new.iterrows():\n",
        "        if isinstance(row['resumen'], str) and pd.notna(row['resumen']):\n",
        "            prediccion = model.predict([row['resumen']])[0]\n",
        "            tono_predicho = {1: 'Positivo', 0: 'Neutro', -1: 'Negativo'}.get(prediccion, 'Desconocido')\n",
        "        else:\n",
        "            prediccion = np.nan\n",
        "            tono_predicho = 'Inválido'\n",
        "\n",
        "        resultados.append({\n",
        "            'resumen': row['resumen'],\n",
        "            'tono_numerico_predicho': prediccion,\n",
        "            'tono_predicho': tono_predicho\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resultados)\n",
        "\n",
        "# Cargar y preprocesar datos de entrenamiento\n",
        "print(\"Subir archivo de entrenamiento:\")\n",
        "uploaded_file = files.upload()\n",
        "file_name = next(iter(uploaded_file))\n",
        "df_train = cargar_y_preprocesar_datos(file_name)\n",
        "\n",
        "print(f\"Tamaño del dataset de entrenamiento: {len(df_train)}\")\n",
        "\n",
        "# Entrenar el modelo\n",
        "modelo = entrenar_modelo(df_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "RiMuPjvs3sUS",
        "outputId": "c3938118-45e6-40da-9675-6053fc1b22ed",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Subir excel con nuevas notas (Concatenar Título y resumen)\n",
        "# Analizar nuevo dataset\n",
        "print(\"\\nSubir archivo para análisis:\")\n",
        "uploaded_new_data = files.upload()\n",
        "new_data_file_name = next(iter(uploaded_new_data))\n",
        "df_resultados = analizar_nuevo_dataset(modelo, new_data_file_name)\n",
        "\n",
        "# Guardar resultados\n",
        "output_file_path = 'resultado_sentimiento.xlsx'\n",
        "df_resultados.to_excel(output_file_path, index=False)\n",
        "print(f\"\\nResultados guardados en: {output_file_path}\")\n",
        "\n",
        "# Mostrar los primeros resultados\n",
        "print(\"\\nPrimeros resultados:\")\n",
        "print(df_resultados.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "rUhPQ2nt3uBz",
        "outputId": "482375f2-a206-42c9-d99f-128713191fab",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "vGaWyEIR8jFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tema"
      ],
      "metadata": {
        "id": "Ex784tRAlG5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Instalación\n",
        "!pip install --quiet --upgrade fasttext nltk openpyxl tqdm joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep4h32oZ14IC",
        "outputId": "72635a3e-a868-44fa-c7d0-5e65afe6f378",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Importaciones y utilidades comunes\n",
        "import os, shutil, re\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# --- Configurar NLTK en un dir. temporal (evita warnings en Colab) ---\n",
        "nltk_data_dir = '/tmp/nltk_data'\n",
        "shutil.rmtree(nltk_data_dir, ignore_errors=True)\n",
        "os.makedirs(nltk_data_dir, exist_ok=True)\n",
        "nltk.data.path = [nltk_data_dir]\n",
        "nltk.download('stopwords', download_dir=nltk_data_dir)\n",
        "\n",
        "# --- Recursos para el pre-procesado ---\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "token_pattern = re.compile(r\"\\b\\w+\\b\", flags=re.UNICODE)   # tokenización rápida por regex\n",
        "\n",
        "def preprocess(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    tokens = token_pattern.findall(text.lower())\n",
        "    return \" \".join(tok for tok in tokens if tok not in stop_words)\n",
        "\n",
        "def preprocess_series(serie, desc=\"Procesando\") -> pd.Series:\n",
        "    \"\"\"Pre-procesa una Serie de textos en paralelo (tqdm + joblib).\"\"\"\n",
        "    tqdm.pandas(desc=desc)\n",
        "    n_jobs = -1   # usa todos los núcleos disponibles\n",
        "    processed = Parallel(n_jobs=n_jobs, backend=\"loky\")(\n",
        "        delayed(preprocess)(txt) for txt in tqdm(serie, desc=desc)\n",
        "    )\n",
        "    return pd.Series(processed, index=serie.index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAsIj9FQ2B9A",
        "outputId": "6f4d93e3-57a6-4c7a-d820-120990382ae1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cargar/limpiar datos de entrenamiento\n",
        "def preprocess_series(series, desc=\"Procesando\"):\n",
        "    from tqdm.notebook import tqdm  # o simplemente `from tqdm import tqdm`\n",
        "    tqdm.pandas(desc=desc)\n",
        "\n",
        "    # Ajusta esta función a lo que necesites hacer por resumen\n",
        "    def procesar_texto(texto):\n",
        "        # Ejemplo de pre-procesamiento (ajústalo a tus necesidades)\n",
        "        texto = texto.lower()\n",
        "        texto = texto.strip()\n",
        "        return texto\n",
        "\n",
        "    return series.progress_apply(procesar_texto)\n"
      ],
      "metadata": {
        "id": "JuZB0wp3fcJq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cargar y evidencia el xlsx de entrenamiento\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "train_path = Path(\"nissan entreno.xlsx\")\n",
        "use_cols   = [\"resumen\", \"tema\"]\n",
        "\n",
        "df = pd.read_excel(train_path, usecols=use_cols, engine=\"openpyxl\")\n",
        "df[\"resumen_procesado\"] = preprocess_series(df[\"resumen\"], desc=\"Tokenizando entreno\")\n",
        "\n",
        "print(\"✔ Entreno cargado y pre-procesado — filas:\", len(df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "wA0J-p4zfroC",
        "outputId": "d05a1709-ff15-4595-a116-4d75933f83f6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Entrenamiento del modelo\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# --- División train/test ---\n",
        "X_train_txt, X_test_txt, y_train, y_test = train_test_split(\n",
        "    df[\"resumen_procesado\"], df[\"tema\"],\n",
        "    test_size=0.20, random_state=42, stratify=df[\"tema\"]\n",
        ")\n",
        "\n",
        "# --- Vectorizador y Naive Bayes ---\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(X_train_txt)\n",
        "X_test  = vectorizer.transform(X_test_txt)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- Evaluación ---\n",
        "pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
        "print(classification_report(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6jLO05G2GIY",
        "outputId": "2a5fdad8-8b2c-46e0-c0e0-1f447b0807b8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Guardar artefactos para producción\n",
        "import joblib\n",
        "\n",
        "joblib.dump(model,      \"modelo_naive_bayes.pkl\")\n",
        "joblib.dump(vectorizer, \"vectorizador.pkl\")\n",
        "print(\"✔ Modelo y vectorizador guardados en disco\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oiTz2zZ2HnI",
        "outputId": "95e6cda5-7014-4b0c-b811-6f03c6522a1c",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Predicción sobre nuevo lote de noticias\n",
        "# --- Cargar artefactos guardados ---\n",
        "loaded_model      = joblib.load(\"modelo_naive_bayes.pkl\")\n",
        "loaded_vectorizer = joblib.load(\"vectorizador.pkl\")\n",
        "\n",
        "# --- Cargar archivo de prueba ---\n",
        "test_path = Path(\"nissan test.xlsx\")\n",
        "nuevas_df = pd.read_excel(test_path, engine=\"openpyxl\")\n",
        "\n",
        "# --- Pre-procesar ---\n",
        "nuevas_df[\"resumen_procesado\"] = preprocess_series(\n",
        "    nuevas_df[\"resumen\"], desc=\"Tokenizando test\"\n",
        ")\n",
        "\n",
        "# --- Vectorizar y predecir ---\n",
        "X_new = loaded_vectorizer.transform(nuevas_df[\"resumen_procesado\"])\n",
        "nuevas_df[\"tema_predicho\"] = loaded_model.predict(X_new)\n",
        "\n",
        "# --- Exportar resultado ---\n",
        "out_path = \"resultado_temas.xlsx\"\n",
        "nuevas_df.to_excel(out_path, index=False)\n",
        "print(f\"✔ Archivo '{out_path}' guardado con {len(nuevas_df)} filas\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "z42bdITx2I3Q",
        "outputId": "eed10e37-4dbb-4ba8-ee2a-c43166c230b1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "HZtRxD8k4RMW"
      }
    }
  ]
}
