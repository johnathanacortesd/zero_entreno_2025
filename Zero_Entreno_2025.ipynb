{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNB3XZ8bEJ9GHYursskdDuv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnathanacortesd/zero_entreno_2025/blob/main/Zero_Entreno_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tono y tema sin datos de entrenamiento"
      ],
      "metadata": {
        "id": "8T0NTLANOshM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tono"
      ],
      "metadata": {
        "id": "HsLbP1EDcX38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar librerías necesarias de forma silenciosa\n",
        "!pip install -qq sentence-transformers pandas openpyxl tqdm emoji\n",
        "\n",
        "# Importar librerías\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import emoji\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import unicodedata\n",
        "from google.colab import userdata # Import userdata to access secrets if needed for other models\n",
        "\n",
        "# Verificar y configurar el uso de GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "    print(\"Using GPU for computation.\")\n",
        "else:\n",
        "    device = 'cpu'\n",
        "    print(\"Using CPU for computation.\")\n",
        "\n",
        "# Definir las categorías de sentimiento y sus palabras clave\n",
        "# Estas palabras clave son descriptivas y ayudan al modelo a entender el 'centro' de cada sentimiento.\n",
        "# Sin embargo, el modelo ya está pre-entrenado para entender el sentimiento directamente.\n",
        "sentiment_keywords = {\n",
        "    \"Positivo\": [\n",
        "        \"excelente\", \"bueno\", \"genial\", \"positivo\", \"fantástico\", \"maravilloso\",\n",
        "        \"feliz\", \"contento\", \"agradable\", \"éxito\", \"perfecto\", \"gran\", \"magnífico\",\n",
        "        \"óptimo\", \"mejor\", \"acuerdo\", \"aprobación\", \"felicitaciones\", \"elogio\"\n",
        "    ],\n",
        "    \"Neutro\": [\n",
        "        \"neutral\", \"información\", \"hecho\", \"dato\", \"reporte\", \"observación\",\n",
        "        \"análisis\", \"declaración\", \"noticia\", \"comunicado\", \"menciona\", \"describe\",\n",
        "        \"sin emoción\", \"objetivo\", \"estado\", \"situación\", \"acerca de\", \"sobre\"\n",
        "    ],\n",
        "    \"Negativo\": [\n",
        "        \"malo\", \"terrible\", \"negativo\", \"problema\", \"crisis\", \"difícil\", \"triste\",\n",
        "        \"decepcionante\", \"preocupante\", \"fallo\", \"error\", \"desastre\", \"crítica\",\n",
        "        \"queja\", \"inconformidad\", \"pésimo\", \"peor\", \"desacuerdo\", \"rechazo\", \"malestar\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# No verificamos palabras clave duplicadas para sentimiento ya que las superposiciones son más comunes\n",
        "# y el modelo se basa más en el significado contextual.\n",
        "\n",
        "# Cargar textos a clasificar\n",
        "from google.colab import files\n",
        "print(\"Sube el archivo con los textos a clasificar:\")\n",
        "uploaded_texts = files.upload()\n",
        "\n",
        "texts_file_name = list(uploaded_texts.keys())[0]\n",
        "texts_df = pd.read_excel(texts_file_name)\n",
        "\n",
        "if 'resumen' not in texts_df.columns:\n",
        "    raise ValueError(\"El archivo a clasificar debe contener una columna 'resumen'.\")\n",
        "\n",
        "# Cargar el modelo de Sentence-BERT específico para análisis de sentimiento en español\n",
        "# 'pysentimiento/robertuito-sentiment-analysis' es un modelo robusto y público para español.\n",
        "# Este modelo NO requiere token de Hugging Face.\n",
        "print(f\"Loading sentiment analysis model to {device}...\")\n",
        "sentiment_model = SentenceTransformer('pysentimiento/robertuito-sentiment-analysis', device=device)\n",
        "print(\"Sentiment analysis model loaded successfully.\")\n",
        "\n",
        "# Crear embeddings de las categorías de sentimiento\n",
        "# Aunque el modelo ya está pre-entrenado, usar estos embeddings como 'prototipos'\n",
        "# ayuda a guiar la clasificación por similitud cosenoidal.\n",
        "sentiment_embeddings = {}\n",
        "for sentiment, keywords in sentiment_keywords.items():\n",
        "    embeddings = sentiment_model.encode(keywords, convert_to_tensor=True)\n",
        "    sentiment_embedding = embeddings.mean(dim=0)\n",
        "    sentiment_embeddings[sentiment] = sentiment_embedding.to(device)\n",
        "\n",
        "# Apilar todos los embeddings de sentimiento en un solo tensor\n",
        "sentiment_embeddings_tensor = torch.stack(list(sentiment_embeddings.values()))\n",
        "sentiment_names = list(sentiment_embeddings.keys())\n",
        "\n",
        "# Función para normalizar el texto (la misma que antes)\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "    return text\n",
        "\n",
        "# Función para preprocesar el texto (la misma que antes)\n",
        "def preprocess_text(text):\n",
        "    text = normalize_text(text)\n",
        "    # Eliminar URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Eliminar caracteres especiales (excepto hashtags si fueran relevantes, pero para sentimiento no suelen serlo)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # Simplificado para sentimiento\n",
        "    return text\n",
        "\n",
        "# Función de clasificación de sentimiento\n",
        "# Ahora usamos un enfoque de 'coincidencia estricta' para palabras clave de sentimiento\n",
        "# y luego la similitud del modelo para el resto.\n",
        "def classify_sentiment(text, model, sentiment_embeddings_tensor, sentiment_names, threshold=0.6, min_score_diff=0.1):\n",
        "    text_preprocessed = preprocess_text(text)\n",
        "\n",
        "    # 1. Búsqueda de coincidencia exacta de palabras clave de sentimiento (alta confianza)\n",
        "    # Esta es una heurística fuerte para asegurar que \"excelente\" siempre sea Positivo.\n",
        "    for sentiment, keywords in sentiment_keywords.items():\n",
        "        for kw in keywords:\n",
        "            if normalize_text(kw) in text_preprocessed.split(): # Split para asegurar palabra completa\n",
        "                return sentiment, 1.0\n",
        "\n",
        "    # 2. Clasificación por similitud con embeddings de categorías (si no hay coincidencia exacta)\n",
        "    text_embedding = model.encode(text_preprocessed, convert_to_tensor=True).to(device)\n",
        "    cos_similarities = util.cos_sim(text_embedding, sentiment_embeddings_tensor)[0]\n",
        "\n",
        "    top_idx = torch.argmax(cos_similarities).item()\n",
        "    top_sentiment = sentiment_names[top_idx]\n",
        "    top_score = cos_similarities[top_idx].item()\n",
        "\n",
        "    # Evaluar la diferencia con la segunda mejor puntuación para mayor precisión\n",
        "    sorted_scores, _ = torch.sort(cos_similarities, descending=True)\n",
        "    if len(sorted_scores) > 1:\n",
        "        second_score = sorted_scores[1].item()\n",
        "        score_diff = top_score - second_score\n",
        "    else:\n",
        "        score_diff = top_score # Si solo hay una categoría, la diferencia es la propia puntuación\n",
        "\n",
        "    # Aplicar umbrales para decidir la confianza o si es \"Neutro\" por ambigüedad\n",
        "    if top_score >= threshold and score_diff >= min_score_diff:\n",
        "        return top_sentiment, top_score\n",
        "    else:\n",
        "        # Si la confianza no es alta o la diferencia es baja,\n",
        "        # puede indicar que el texto es más neutro o ambiguo.\n",
        "        # Podríamos forzar a \"Neutro\" o dejar la categoría de mayor similitud\n",
        "        # con una confianza menor. Optamos por dejar la de mayor similitud.\n",
        "        return top_sentiment, top_score\n",
        "\n",
        "\n",
        "# Clasificar textos y almacenar resultados\n",
        "texts = texts_df['resumen'].fillna(\"\").tolist()\n",
        "results = []\n",
        "\n",
        "# Definir umbrales de confianza para el análisis de sentimiento\n",
        "sentiment_threshold = 0.65  # Umbral de confianza para la clasificación de sentimiento\n",
        "sentiment_min_score_diff = 0.15 # Diferencia mínima entre top 1 y top 2 para clasificación clara\n",
        "\n",
        "# Procesamiento por lotes\n",
        "batch_size = 32 # Ajusta este valor según la memoria de la GPU\n",
        "num_batches = (len(texts) + batch_size - 1) // batch_size\n",
        "\n",
        "for i in tqdm(range(num_batches), desc=\"Clasificando sentimiento por lotes\"):\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = min((i + 1) * batch_size, len(texts))\n",
        "    batch_texts = texts[start_idx:end_idx]\n",
        "\n",
        "    # Preprocesar textos del lote (directamente aquí para evitar re-preprocesar en la función)\n",
        "    batch_preprocessed_texts = [preprocess_text(text) for text in batch_texts]\n",
        "\n",
        "    # Codificar todos los textos del lote a la vez para eficiencia en GPU\n",
        "    # Nota: No pasamos el token aquí, ya que 'pysentimiento/robertuito-sentiment-analysis' es público\n",
        "    batch_text_embeddings = sentiment_model.encode(batch_preprocessed_texts, convert_to_tensor=True).to(device)\n",
        "\n",
        "    for j, original_text in enumerate(batch_texts):\n",
        "        # Obtener el embedding pre-calculado para el texto actual del lote\n",
        "        current_text_embedding = batch_text_embeddings[j]\n",
        "        current_text_preprocessed = batch_preprocessed_texts[j]\n",
        "\n",
        "        # 1. Coincidencia exacta de palabras clave de sentimiento (prioridad alta)\n",
        "        found_exact_match = False\n",
        "        for sentiment, keywords in sentiment_keywords.items():\n",
        "            for kw in keywords:\n",
        "                # Usar .split() para asegurar que 'bueno' en 'muy bueno' no coincida con 'buen'\n",
        "                if normalize_text(kw) in current_text_preprocessed.split():\n",
        "                    results.append({\n",
        "                        \"texto\": original_text,\n",
        "                        \"sentimiento\": sentiment,\n",
        "                        \"confianza\": 1.0 # Confianza máxima si hay coincidencia exacta\n",
        "                    })\n",
        "                    found_exact_match = True\n",
        "                    break\n",
        "            if found_exact_match:\n",
        "                break\n",
        "\n",
        "        if found_exact_match:\n",
        "            continue # Si ya clasificamos por coincidencia exacta, pasamos al siguiente texto\n",
        "\n",
        "        # 2. Clasificación por similitud cosenoidal (para el resto de textos)\n",
        "        cos_similarities = util.cos_sim(current_text_embedding, sentiment_embeddings_tensor)[0]\n",
        "\n",
        "        top_idx = torch.argmax(cos_similarities).item()\n",
        "        top_sentiment = sentiment_names[top_idx]\n",
        "        top_score = cos_similarities[top_idx].item()\n",
        "\n",
        "        # Obtener la segunda mayor similitud para evaluar la diferencia\n",
        "        sorted_scores, _ = torch.sort(cos_similarities, descending=True)\n",
        "        if len(sorted_scores) > 1:\n",
        "            second_score = sorted_scores[1].item()\n",
        "            score_diff = top_score - second_score\n",
        "        else:\n",
        "            score_diff = top_score # Si solo hay una categoría, la diferencia es la puntuación misma\n",
        "\n",
        "        # Decidir la categoría basada en el umbral y la diferencia de puntuaciones\n",
        "        if top_score >= sentiment_threshold and score_diff >= sentiment_min_score_diff:\n",
        "            final_sentiment = top_sentiment\n",
        "            final_confidence = top_score\n",
        "        else:\n",
        "            # Si la confianza no es lo suficientemente alta o hay mucha ambigüedad,\n",
        "            # lo asignamos como Neutro para ser más conservadores, o la de mayor similitud\n",
        "            # con su score.\n",
        "            # Aquí, priorizamos la categoría de mayor similitud si no cumple los umbrales\n",
        "            # de \"alta confianza\" pero aún así necesitamos una clasificación.\n",
        "            final_sentiment = top_sentiment # O podrías forzar a \"Neutro\" aquí si prefieres.\n",
        "            final_confidence = top_score\n",
        "\n",
        "        results.append({\n",
        "            \"texto\": original_text,\n",
        "            \"sentimiento\": final_sentiment,\n",
        "            \"confianza\": final_confidence\n",
        "        })\n",
        "\n",
        "# Exportar resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_excel(\"sentiment_classification_results.xlsx\", index=False)\n",
        "print(\"Clasificación de sentimiento completada. Descarga el archivo:\")\n",
        "files.download(\"sentiment_classification_results.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404,
          "referenced_widgets": [
            "506f93d25b23436a82bbce5205f98753",
            "6a0bc59145804a49bb4bdb27b9dd4c79",
            "7b3f6c071f6146a0b294e908eb6a2ca8",
            "332e38d6632943b3800eba7ca7c5e1a8",
            "717be8b2afba4d13b629c39d67c5451a",
            "335cb6f1ca064fc0852879872f317faa",
            "2a57ea9bc72f43a88ffa8b95d3b93980",
            "b380d136c6d3492796f424d89d7be99d",
            "cafdda955f0944c98414cac856bcf9cd",
            "23164914d7254f06988d4c62b5e305f3",
            "1c9f9b88a377461ea3d1627e358ab882",
            "11d3f21818ef4fc69ff5a15ed7a09bde",
            "6685a1e473b04a39832170c89b344d9e",
            "8961460d9a594be0ac11f59cb91b7136",
            "db5645643eb448d79763e39f4efb1895",
            "f62ed301431f4e9ba8331ff6541c267e",
            "50112c8167b042de9a915e55db18e648",
            "37a40458bb5648a999ade65a2cbc7d2e",
            "27c243eedcf946b98d7e42d6718aae39",
            "99d4a288f5ee4ef8a85c59fb716313e6",
            "9957924d0c384c8d8d5407d6124c1d82",
            "7013ce659a654a8c8c643c610eddf2b6",
            "e240ed9e972543459d78a0539bc2ed54",
            "aa81ae1af65a478693bc69ffcfe82c72",
            "f041f4c06b0d438c9d8da202b7e1c39b",
            "9e597807f22b40f3abd2e01b9e406d28",
            "52fdc62e977c48038c99a6b606197654",
            "1b4bb93bfb5d43c6ad5d7c855a7e21af",
            "1aaf55cb3fa84c9e8d9c5818f9d57813",
            "c4d80c00c9eb44c18efab5732e062433",
            "21390f74b1484f649b53d9755e02df6a",
            "694452b83acd47549cb2c593e6f3e403",
            "d1f59330e8bd4327bfc999de77de36ac",
            "6b92e5f1e9114ee69cea347bf8ad6018",
            "11d2b09492cd4c0f88811b9625cd36ff",
            "2b27d2dbaca74ef1889e32745f43105f",
            "cdd74a1fa6924120a1701e7fac62bc4d",
            "1d14f252eeaa46449a1045e2c3c8de0c",
            "42c78c8e639a4b12bbcd8ed6d1d5df24",
            "2971435e57304f65ae267fc1a919fe01",
            "3c14e45915d24f82932eedfbed0f7521",
            "612a10744c59464c93779e2171d51616",
            "4cc4bc9e7680449388b73221cf9f1b81",
            "4aa2ed636b1947fab1c63ec86e4c73ed",
            "97efba92d06946c2a4fd37439b1abdc5",
            "42752ddbb7244c12ada6e5c81fadff47",
            "54c663afbd824bdf989458864fa727f3",
            "11ddfa3e41d34fb383618dc3c137d67a",
            "acc1623da103424bae439cfabc0ddae7",
            "69e9d46b30f54af4a9e5789bc6f97fb3",
            "289d67857e8f433eb85cbeee6d660b35",
            "a3b609609ad74a5aa126977c5f06d5c7",
            "4bd27eb0acb240d690b4a4ee49c02648",
            "c61dcdc9aa96474a93e36bedfbbca19a",
            "94a26ad05de94059a2e36575e1abc8be",
            "e91fa2f2a1624555ad54dbad5a3b72f7",
            "ea518b507d494b1aa7b5355bd6ee47c6",
            "72938624eb7848d28bf15414d108ad9c",
            "2f19ca8db9444fc7b23d0843d451343e",
            "ffc8c23256b94f4fad0fcc7ca9f45d29",
            "7eb99b4f5ad84b72a830920531777d0f",
            "572e926c795c46ef870ca392e561f3b5",
            "f20f8769c204480ab634a4713193c3d3",
            "ad635941cd0b4d91a83c8d199053f4f0",
            "8924d22c27144ba593ba858efc4829f1",
            "eef853858c9f43d09c8a0dff143e2019"
          ]
        },
        "id": "uSYKtZU1ceeK",
        "outputId": "60fd9bad-af89-4e54-c50a-4add458b78d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU for computation.\n",
            "Sube el archivo con los textos a clasificar:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-46cdd0c8-95d2-4413-8a52-f179d356c5e3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-46cdd0c8-95d2-4413-8a52-f179d356c5e3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving nissan test.xlsx to nissan test (8).xlsx\n",
            "Loading sentiment analysis model to cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name pysentimiento/robertuito-sentiment-analysis. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/925 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "506f93d25b23436a82bbce5205f98753"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/435M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11d3f21818ef4fc69ff5a15ed7a09bde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at pysentimiento/robertuito-sentiment-analysis and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e240ed9e972543459d78a0539bc2ed54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b92e5f1e9114ee69cea347bf8ad6018"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97efba92d06946c2a4fd37439b1abdc5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis model loaded successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Clasificando sentimiento por lotes:   0%|          | 0/12 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e91fa2f2a1624555ad54dbad5a3b72f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clasificación de sentimiento completada. Descarga el archivo:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b983441a-e36a-4e3d-92d2-85fdebd4f0c5\", \"sentiment_classification_results.xlsx\", 65804)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tema"
      ],
      "metadata": {
        "id": "VJY1yHxSdAwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar librerías necesarias de forma silenciosa\n",
        "!pip install -qq sentence-transformers pandas openpyxl tqdm emoji\n",
        "\n",
        "# Importar librerías\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import emoji\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import unicodedata\n",
        "from google.colab import userdata # Import userdata to access secrets (still good practice for other needs)\n",
        "\n",
        "# --- Accessing Hugging Face token (no longer strictly necessary for this model, but kept for general good practice) ---\n",
        "# You can remove this block if you're sure you won't need any private models in the future.\n",
        "# However, it's harmless to keep it for general authentication purposes.\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    if hf_token is None:\n",
        "        print(\"HF_TOKEN secret not found. This is okay for the chosen public model.\")\n",
        "    else:\n",
        "        print(\"Hugging Face token accessed from Colab Secrets (not strictly needed for this model).\")\n",
        "except Exception as e:\n",
        "    print(f\"Error accessing HF_TOKEN secret: {e}. This is okay for the chosen public model.\")\n",
        "    hf_token = None\n",
        "\n",
        "\n",
        "# Verificar uso de GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "    print(\"Using GPU for computation.\")\n",
        "else:\n",
        "    device = 'cpu'\n",
        "    print(\"Using CPU for computation.\")\n",
        "\n",
        "# Definir 10 temas muy genéricos para análisis de contenido\n",
        "predefined_keywords = {\n",
        "    \"Noticias y Eventos Actuales\": [\n",
        "        \"noticia\", \"actualidad\", \"evento\", \"suceso\", \"última hora\", \"acontecimiento\",\n",
        "        \"incidente\", \"desarrollo\", \"informe\", \"periodismo\", \"reportaje\", \"cobertura\",\n",
        "        \"reciente\", \"hoy\", \"ayer\"\n",
        "    ],\n",
        "    \"Opiniones y Debates\": [\n",
        "        \"opinión\", \"debate\", \"discusión\", \"punto de vista\", \"perspectiva\", \"argumento\",\n",
        "        \"controversia\", \"polémica\", \"crítica\", \"elogio\", \"comentario\", \"sentimiento\",\n",
        "        \"postura\", \"análisis\", \"reflexión\"\n",
        "    ],\n",
        "    \"Productos y Servicios\": [\n",
        "        \"producto\", \"servicio\", \"marca\", \"empresa\", \"oferta\", \"lanzamiento\",\n",
        "        \"característica\", \"función\", \"precio\", \"disponibilidad\", \"calidad\",\n",
        "        \"experiencia\", \"uso\", \"beneficio\", \"solución\"\n",
        "    ],\n",
        "    \"Economía y Negocios\": [\n",
        "        \"economía\", \"negocio\", \"mercado\", \"inversión\", \"finanzas\", \"empresa\",\n",
        "        \"crecimiento\", \"crisis económica\", \"sector\", \"comercio\", \"desempleo\",\n",
        "        \"tendencia económica\", \"capital\", \"ingresos\", \"gastos\"\n",
        "    ],\n",
        "    \"Política y Gobierno\": [\n",
        "        \"política\", \"gobierno\", \"ley\", \"regulación\", \"decisión\", \"elecciones\",\n",
        "        \"partido\", \"líder\", \"democracia\", \"justicia\", \"reforma\", \"público\",\n",
        "        \"estado\", \"norma\", \"oficial\"\n",
        "    ],\n",
        "    \"Sociedad y Cultura\": [\n",
        "        \"sociedad\", \"cultura\", \"comunidad\", \"educación\", \"salud\", \"arte\",\n",
        "        \"historia\", \"valores\", \"tendencia social\", \"grupo\", \"tradición\",\n",
        "        \"identidad\", \"cambio social\", \"bienestar\", \"equidad\"\n",
        "    ],\n",
        "    \"Tecnología e Innovación\": [\n",
        "        \"tecnología\", \"innovación\", \"digital\", \"internet\", \"software\", \"hardware\",\n",
        "        \"futuro\", \"desarrollo\", \"descubrimiento\", \"aplicación\", \"sistema\",\n",
        "        \"robótica\", \"inteligencia artificial\", \"conectividad\", \"ciberseguridad\"\n",
        "    ],\n",
        "    \"Medio Ambiente y Sostenibilidad\": [\n",
        "        \"medio ambiente\", \"sostenibilidad\", \"cambio climático\", \"ecología\",\n",
        "        \"contaminación\", \"recursos naturales\", \"biodiversidad\", \"energía\",\n",
        "        \"conservación\", \"reciclaje\", \"impacto ambiental\", \"verde\", \"planeta\",\n",
        "        \"residuos\"\n",
        "    ],\n",
        "    \"Salud y Bienestar\": [\n",
        "        \"salud\", \"bienestar\", \"enfermedad\", \"tratamiento\", \"medicina\", \"hospital\",\n",
        "        \"cuidado\", \"prevención\", \"alimentación\", \"ejercicio\", \"salud mental\",\n",
        "        \"diagnóstico\", \"síntoma\", \"recuperación\", \"terapia\"\n",
        "    ],\n",
        "    \"Deportes y Entretenimiento\": [\n",
        "        \"deporte\", \"entretenimiento\", \"película\", \"música\", \"serie\", \"juego\",\n",
        "        \"artista\", \"celebridad\", \"evento deportivo\", \"cultura pop\", \"diversión\",\n",
        "        \"espectáculo\", \"campeonato\", \"afición\", \"creativo\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Verificar que no haya palabras clave duplicadas entre categorías\n",
        "keyword_to_topics = defaultdict(list)\n",
        "for topic, keywords in predefined_keywords.items():\n",
        "    for kw in keywords:\n",
        "        keyword_to_topics[kw.lower()].append(topic)\n",
        "\n",
        "# Encontrar palabras clave que se repiten en múltiples categorías\n",
        "duplicate_keywords = {kw: topics for kw, topics in keyword_to_topics.items() if len(topics) > 1}\n",
        "\n",
        "if duplicate_keywords:\n",
        "    print(\"Palabras clave duplicadas encontradas:\")\n",
        "    for kw, topics in duplicate_keywords.items():\n",
        "        print(f\"'{kw}' se encuentra en las categorías: {topics}\")\n",
        "else:\n",
        "    print(\"No se encontraron palabras clave duplicadas entre categorías.\")\n",
        "\n",
        "# Cargar textos a clasificar\n",
        "from google.colab import files\n",
        "print(\"Sube el archivo con los textos a clasificar:\")\n",
        "uploaded_texts = files.upload()\n",
        "\n",
        "texts_file_name = list(uploaded_texts.keys())[0]\n",
        "texts_df = pd.read_excel(texts_file_name)\n",
        "\n",
        "if 'resumen' not in texts_df.columns:\n",
        "    raise ValueError(\"El archivo a clasificar debe contener una columna 'resumen'.\")\n",
        "\n",
        "# Cargar el modelo de Sentence-BERT (AHORA USANDO UN MODELO PÚBLICO Y ACCESIBLE)\n",
        "embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2', device=device) # Removed 'token=hf_token'\n",
        "\n",
        "# Crear embeddings de temas\n",
        "topic_embeddings = {}\n",
        "for topic, keywords in predefined_keywords.items():\n",
        "    if keywords:\n",
        "        embeddings = embedding_model.encode(keywords, convert_to_tensor=True)\n",
        "        topic_embedding = embeddings.mean(dim=0)\n",
        "    else:\n",
        "        topic_embedding = torch.zeros(embedding_model.get_sentence_embedding_dimension())\n",
        "    topic_embeddings[topic] = topic_embedding.to(device)\n",
        "\n",
        "# Apilar todos los embeddings de temas en un solo tensor\n",
        "topic_embeddings_tensor = torch.stack(list(topic_embeddings.values()))\n",
        "topic_names = list(topic_embeddings.keys())\n",
        "\n",
        "# Función para normalizar el texto\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "    return text\n",
        "\n",
        "# Función para preprocesar el texto\n",
        "def preprocess_text(text):\n",
        "    text = normalize_text(text)\n",
        "    # Eliminar URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Eliminar caracteres especiales excepto hashtags\n",
        "    text = re.sub(r'[^\\w#\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Función para verificar coincidencia exacta de palabras clave\n",
        "def exact_match_all(text, predefined_keywords):\n",
        "    matched_categories = []\n",
        "    # Tokenizar el texto para incluir palabras y hashtags\n",
        "    tokens = re.findall(r'#\\w+|\\w+', text)\n",
        "    # Normalizar tokens\n",
        "    normalized_tokens = [normalize_text(tok) for tok in tokens]\n",
        "\n",
        "    for topic, keywords in predefined_keywords.items():\n",
        "        for kw in keywords:\n",
        "            kw_normalized = normalize_text(kw)\n",
        "            if kw_normalized in normalized_tokens:\n",
        "                matched_categories.append(topic)\n",
        "                break\n",
        "    return matched_categories\n",
        "\n",
        "# Clasificar textos\n",
        "texts = texts_df['resumen'].fillna(\"\").tolist()\n",
        "results = []\n",
        "\n",
        "# Definir umbral de confianza y diferencia mínima para clasificación\n",
        "threshold = 0.65  # Mantener un umbral alto para precisión\n",
        "min_score_diff = 0.15 # Mantener una buena diferencia para desempate claro\n",
        "\n",
        "# Procesamiento por lotes\n",
        "batch_size = 32 # Ajusta este valor según la memoria de la GPU\n",
        "num_batches = (len(texts) + batch_size - 1) // batch_size\n",
        "\n",
        "for i in tqdm(range(num_batches), desc=\"Clasificando textos por lotes\"):\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = min((i + 1) * batch_size, len(texts))\n",
        "    batch_texts = texts[start_idx:end_idx]\n",
        "\n",
        "    batch_preprocessed_texts = [preprocess_text(text) for text in batch_texts]\n",
        "\n",
        "    # Codificar todos los textos del lote a la vez\n",
        "    batch_embeddings = embedding_model.encode(batch_preprocessed_texts, convert_to_tensor=True).to(device)\n",
        "\n",
        "    for j, original_text in enumerate(batch_texts):\n",
        "        text_preprocessed = batch_preprocessed_texts[j]\n",
        "        text_embedding = batch_embeddings[j]\n",
        "\n",
        "        # Buscar coincidencias de palabras clave primero\n",
        "        matched_categories = exact_match_all(text_preprocessed, predefined_keywords)\n",
        "\n",
        "        if matched_categories:\n",
        "            from collections import Counter\n",
        "            category_counts = Counter(matched_categories)\n",
        "            top_category = category_counts.most_common(1)[0][0]\n",
        "            results.append({\n",
        "                \"texto\": original_text,\n",
        "                \"categoria\": top_category,\n",
        "                \"confianza\": 1.0  # Confianza alta si hay coincidencia exacta\n",
        "            })\n",
        "        else:\n",
        "            # Calcular similitudes con todos los temas\n",
        "            cos_similarities = util.cos_sim(text_embedding, topic_embeddings_tensor)[0]\n",
        "\n",
        "            # Obtener el índice del tema con mayor similitud\n",
        "            top_idx = torch.argmax(cos_similarities).item()\n",
        "            top_topic = topic_names[top_idx]\n",
        "            top_score = cos_similarities[top_idx].item()\n",
        "\n",
        "            # Obtener la segunda mayor similitud para evaluar la diferencia\n",
        "            sorted_scores, sorted_indices = torch.sort(cos_similarities, descending=True)\n",
        "            if len(sorted_scores) > 1:\n",
        "                second_score = sorted_scores[1].item()\n",
        "                score_diff = top_score - second_score\n",
        "            else:\n",
        "                score_diff = top_score # Si solo hay un tema, la diferencia es la puntuación misma\n",
        "\n",
        "            # Decidir la categoría basada en el umbral y la diferencia de puntuaciones\n",
        "            if top_score >= threshold and score_diff >= min_score_diff:\n",
        "                categoria = top_topic\n",
        "                confianza = top_score\n",
        "            else:\n",
        "                # Si no cumple los criterios de confianza, se asigna a la categoría de mayor similitud\n",
        "                # pero la confianza reflejará que la correspondencia no fue muy fuerte.\n",
        "                categoria = top_topic\n",
        "                confianza = top_score\n",
        "\n",
        "            results.append({\n",
        "                \"texto\": original_text,\n",
        "                \"categoria\": categoria,\n",
        "                \"confianza\": confianza\n",
        "            })\n",
        "\n",
        "# Exportar resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_excel(\"classification_results.xlsx\", index=False)\n",
        "print(\"Clasificación completada. Descarga el archivo:\")\n",
        "files.download(\"classification_results.xlsx\")"
      ],
      "metadata": {
        "id": "oqWkCtILdBwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tono y tema con datos de entrenamiento"
      ],
      "metadata": {
        "id": "5Iy1p9yeDOF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tono"
      ],
      "metadata": {
        "id": "aNsggWKA3CPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq sentiment-analysis-spanish scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CqgdiVb3D9Q",
        "outputId": "af56abfa-2cc6-41d4-b9d6-9a84e573cef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentiment-analysis-spanish\n",
            "  Downloading sentiment_analysis_spanish-0.0.25-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Downloading sentiment_analysis_spanish-0.0.25-py3-none-any.whl (30.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.0/30.0 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentiment-analysis-spanish\n",
            "Successfully installed sentiment-analysis-spanish-0.0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Subir excel de entrenamiento"
      ],
      "metadata": {
        "id": "BTt0l2O73QXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import metrics\n",
        "from google.colab import files\n",
        "\n",
        "def cargar_y_preprocesar_datos(archivo):\n",
        "    df = pd.read_excel(archivo)\n",
        "\n",
        "    # Mapear etiquetas de texto a valores numéricos\n",
        "    label_mapping = {'Positivo': 1, 'Neutro': 0, 'Negativo': -1}\n",
        "    df['tono_numerico'] = df['tono'].map(label_mapping)\n",
        "\n",
        "    # Filtrar filas inválidas\n",
        "    df = df[df['resumen'].notna() & df['resumen'].apply(lambda x: isinstance(x, str))]\n",
        "    df = df[df['tono_numerico'].notna()]\n",
        "\n",
        "    return df\n",
        "\n",
        "def entrenar_modelo(df):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['resumen'], df['tono_numerico'], test_size=0.2, random_state=42)\n",
        "\n",
        "    model = make_pipeline(CountVectorizer(min_df=1), LogisticRegression())\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluar el modelo\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = metrics.accuracy_score(y_test, predictions)\n",
        "    print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def analizar_nuevo_dataset(model, archivo):\n",
        "    df_new = pd.read_excel(archivo)\n",
        "\n",
        "    # Procesar fila por fila\n",
        "    resultados = []\n",
        "    for idx, row in df_new.iterrows():\n",
        "        if isinstance(row['resumen'], str) and pd.notna(row['resumen']):\n",
        "            prediccion = model.predict([row['resumen']])[0]\n",
        "            tono_predicho = {1: 'Positivo', 0: 'Neutro', -1: 'Negativo'}.get(prediccion, 'Desconocido')\n",
        "        else:\n",
        "            prediccion = np.nan\n",
        "            tono_predicho = 'Inválido'\n",
        "\n",
        "        resultados.append({\n",
        "            'resumen': row['resumen'],\n",
        "            'tono_numerico_predicho': prediccion,\n",
        "            'tono_predicho': tono_predicho\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(resultados)\n",
        "\n",
        "# Cargar y preprocesar datos de entrenamiento\n",
        "print(\"Subir archivo de entrenamiento:\")\n",
        "uploaded_file = files.upload()\n",
        "file_name = next(iter(uploaded_file))\n",
        "df_train = cargar_y_preprocesar_datos(file_name)\n",
        "\n",
        "print(f\"Tamaño del dataset de entrenamiento: {len(df_train)}\")\n",
        "\n",
        "# Entrenar el modelo\n",
        "modelo = entrenar_modelo(df_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "RiMuPjvs3sUS",
        "outputId": "c3938118-45e6-40da-9675-6053fc1b22ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subir archivo de entrenamiento:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f2c5a19a-d52a-4034-baa7-d8c02fd9b91a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f2c5a19a-d52a-4034-baa7-d8c02fd9b91a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving nissan entreno.xlsx to nissan entreno.xlsx\n",
            "Tamaño del dataset de entrenamiento: 90043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Subir excel con nuevas notas (Concatenar Título y resumen)\n",
        "####Ejemplo +concat G2;\" \";R2"
      ],
      "metadata": {
        "id": "85-wp1nf3VGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analizar nuevo dataset\n",
        "print(\"\\nSubir archivo para análisis:\")\n",
        "uploaded_new_data = files.upload()\n",
        "new_data_file_name = next(iter(uploaded_new_data))\n",
        "df_resultados = analizar_nuevo_dataset(modelo, new_data_file_name)\n",
        "\n",
        "# Guardar resultados\n",
        "output_file_path = 'resultado_sentimiento.xlsx'\n",
        "df_resultados.to_excel(output_file_path, index=False)\n",
        "print(f\"\\nResultados guardados en: {output_file_path}\")\n",
        "\n",
        "# Mostrar los primeros resultados\n",
        "print(\"\\nPrimeros resultados:\")\n",
        "print(df_resultados.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "rUhPQ2nt3uBz",
        "outputId": "482375f2-a206-42c9-d99f-128713191fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Subir archivo para análisis:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b53c1885-cfab-44f1-83da-c72a60bd62dc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b53c1885-cfab-44f1-83da-c72a60bd62dc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving nissan test (1).xlsx to nissan test (1).xlsx\n",
            "\n",
            "Resultados guardados en: resultado_sentimiento.xlsx\n",
            "\n",
            "Primeros resultados:\n",
            "                                             resumen  tono_numerico_predicho  \\\n",
            "0  Renault-Sofasa, empresa automotriz más atracti...                       1   \n",
            "1  DIÉSELO GASOLINA, CUÁL ELEGIR FINCA PARA LA FI...                       1   \n",
            "2  DIÉSELO GASOLINA, CUÁL ELEGIR FINCA PARA LA FI...                       1   \n",
            "3  DIÉSELO GASOLINA, CUÁL ELEGIR FINCA PARA LA FI...                       1   \n",
            "4  DIÉSELO GASOLINA, CUÁL ELEGIR FINCA PARA LA FI...                       1   \n",
            "\n",
            "  tono_predicho  \n",
            "0      Positivo  \n",
            "1      Positivo  \n",
            "2      Positivo  \n",
            "3      Positivo  \n",
            "4      Positivo  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "vGaWyEIR8jFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tema"
      ],
      "metadata": {
        "id": "Ex784tRAlG5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Instalación (sólo la 1.ª vez)"
      ],
      "metadata": {
        "id": "ZfgXw_Uc7RDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade fasttext nltk openpyxl tqdm joblib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep4h32oZ14IC",
        "outputId": "72635a3e-a868-44fa-c7d0-5e65afe6f378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importaciones y utilidades comunes"
      ],
      "metadata": {
        "id": "KftgD4g77THs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, re\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# --- Configurar NLTK en un dir. temporal (evita warnings en Colab) ---\n",
        "nltk_data_dir = '/tmp/nltk_data'\n",
        "shutil.rmtree(nltk_data_dir, ignore_errors=True)\n",
        "os.makedirs(nltk_data_dir, exist_ok=True)\n",
        "nltk.data.path = [nltk_data_dir]\n",
        "nltk.download('stopwords', download_dir=nltk_data_dir)\n",
        "\n",
        "# --- Recursos para el pre-procesado ---\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "token_pattern = re.compile(r\"\\b\\w+\\b\", flags=re.UNICODE)   # tokenización rápida por regex\n",
        "\n",
        "def preprocess(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    tokens = token_pattern.findall(text.lower())\n",
        "    return \" \".join(tok for tok in tokens if tok not in stop_words)\n",
        "\n",
        "def preprocess_series(serie, desc=\"Procesando\") -> pd.Series:\n",
        "    \"\"\"Pre-procesa una Serie de textos en paralelo (tqdm + joblib).\"\"\"\n",
        "    tqdm.pandas(desc=desc)\n",
        "    n_jobs = -1   # usa todos los núcleos disponibles\n",
        "    processed = Parallel(n_jobs=n_jobs, backend=\"loky\")(\n",
        "        delayed(preprocess)(txt) for txt in tqdm(serie, desc=desc)\n",
        "    )\n",
        "    return pd.Series(processed, index=serie.index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAsIj9FQ2B9A",
        "outputId": "6f4d93e3-57a6-4c7a-d820-120990382ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /tmp/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3##Cargar/limpiar datos de entrenamiento"
      ],
      "metadata": {
        "id": "0l7yr9pA7Yhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_series(series, desc=\"Procesando\"):\n",
        "    from tqdm.notebook import tqdm  # o simplemente `from tqdm import tqdm`\n",
        "    tqdm.pandas(desc=desc)\n",
        "\n",
        "    # Ajusta esta función a lo que necesites hacer por resumen\n",
        "    def procesar_texto(texto):\n",
        "        # Ejemplo de pre-procesamiento (ajústalo a tus necesidades)\n",
        "        texto = texto.lower()\n",
        "        texto = texto.strip()\n",
        "        return texto\n",
        "\n",
        "    return series.progress_apply(procesar_texto)\n"
      ],
      "metadata": {
        "id": "JuZB0wp3fcJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "train_path = Path(\"nissan entreno.xlsx\")\n",
        "use_cols   = [\"resumen\", \"tema\"]\n",
        "\n",
        "df = pd.read_excel(train_path, usecols=use_cols, engine=\"openpyxl\")\n",
        "df[\"resumen_procesado\"] = preprocess_series(df[\"resumen\"], desc=\"Tokenizando entreno\")\n",
        "\n",
        "print(\"✔ Entreno cargado y pre-procesado — filas:\", len(df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f7bdad074a5b4cd88a2dfd15336d72b8",
            "97f71093039c4ba7b591dcca42625bdb",
            "534270c2dd044c70a35c0a1563105aac",
            "18a01d3a2f5f4208a28d84b3b7bf6b14",
            "31b5b6b1882343f08357e34cb7ecc447",
            "edbde889729342899168205c7c8a738a",
            "68bfd34a63c44e758406edf1f1266914",
            "e9c8f69771494173827e287a22d43475",
            "4c60a7491d7f4d938e1247ca6375050f",
            "389c44a0444141bca8a496cc95487910",
            "6acfc455a0d74c7691c181bcbc154550"
          ]
        },
        "id": "wA0J-p4zfroC",
        "outputId": "d05a1709-ff15-4595-a116-4d75933f83f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizando entreno:   0%|          | 0/90043 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7bdad074a5b4cd88a2dfd15336d72b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Entreno cargado y pre-procesado — filas: 90043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "WFxZcoN77dwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# --- División train/test ---\n",
        "X_train_txt, X_test_txt, y_train, y_test = train_test_split(\n",
        "    df[\"resumen_procesado\"], df[\"tema\"],\n",
        "    test_size=0.20, random_state=42, stratify=df[\"tema\"]\n",
        ")\n",
        "\n",
        "# --- Vectorizador y Naive Bayes ---\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(X_train_txt)\n",
        "X_test  = vectorizer.transform(X_test_txt)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- Evaluación ---\n",
        "pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
        "print(classification_report(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6jLO05G2GIY",
        "outputId": "2a5fdad8-8b2c-46e0-c0e0-1f447b0807b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7487367427397412\n",
            "                                   precision    recall  f1-score   support\n",
            "\n",
            "    Activaciones locales de marca       0.84      0.09      0.16       175\n",
            "                         Deportes       0.90      0.87      0.88       451\n",
            "                           Diseño       0.71      0.75      0.73      2377\n",
            "                  Electrificación       0.63      0.72      0.67      1379\n",
            "Escasez inventarios - componentes       0.00      0.00      0.00        21\n",
            "           Estadísticas - Mercado       0.92      0.84      0.88      4819\n",
            "                          Eventos       0.80      0.77      0.78       749\n",
            "            Fábricas - Producción       0.71      0.85      0.77      1467\n",
            "            Influencers - Aliados       0.72      0.49      0.58       734\n",
            "       Investigación y Desarrollo       0.71      0.26      0.38       319\n",
            "                       Judiciales       0.96      0.61      0.74       298\n",
            "                     Lanzamientos       0.67      0.84      0.74      2379\n",
            "                          Mención       0.65      0.56      0.60       819\n",
            "        Premios y Reconocimientos       0.83      0.69      0.75       705\n",
            "          Proyecciones - Balances       0.57      0.70      0.63      1227\n",
            "         Recall - Fallas técnicas       1.00      0.07      0.12        90\n",
            "\n",
            "                         accuracy                           0.75     18009\n",
            "                        macro avg       0.73      0.57      0.59     18009\n",
            "                     weighted avg       0.76      0.75      0.74     18009\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Guardar artefactos para producción"
      ],
      "metadata": {
        "id": "USjROlGo7bsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(model,      \"modelo_naive_bayes.pkl\")\n",
        "joblib.dump(vectorizer, \"vectorizador.pkl\")\n",
        "print(\"✔ Modelo y vectorizador guardados en disco\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oiTz2zZ2HnI",
        "outputId": "95e6cda5-7014-4b0c-b811-6f03c6522a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Modelo y vectorizador guardados en disco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Predicción sobre nuevo lote de noticias"
      ],
      "metadata": {
        "id": "ejPDU8937jrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cargar artefactos guardados ---\n",
        "loaded_model      = joblib.load(\"modelo_naive_bayes.pkl\")\n",
        "loaded_vectorizer = joblib.load(\"vectorizador.pkl\")\n",
        "\n",
        "# --- Cargar archivo de prueba ---\n",
        "test_path = Path(\"nissan test.xlsx\")\n",
        "nuevas_df = pd.read_excel(test_path, engine=\"openpyxl\")\n",
        "\n",
        "# --- Pre-procesar ---\n",
        "nuevas_df[\"resumen_procesado\"] = preprocess_series(\n",
        "    nuevas_df[\"resumen\"], desc=\"Tokenizando test\"\n",
        ")\n",
        "\n",
        "# --- Vectorizar y predecir ---\n",
        "X_new = loaded_vectorizer.transform(nuevas_df[\"resumen_procesado\"])\n",
        "nuevas_df[\"tema_predicho\"] = loaded_model.predict(X_new)\n",
        "\n",
        "# --- Exportar resultado ---\n",
        "out_path = \"resultado_temas.xlsx\"\n",
        "nuevas_df.to_excel(out_path, index=False)\n",
        "print(f\"✔ Archivo '{out_path}' guardado con {len(nuevas_df)} filas\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9c121eaa47af41a79147f0796367a3ad",
            "eff12a21ef9a4bb1b465e5494930beed",
            "09957d713b974059ab0f49806f7ba094",
            "6a348b50d88748ff9bc2b01eaec9fe90",
            "70957f1d408b4094b1d16499d92a9c59",
            "e5dcb3e1f0c846fea4373e105a8426eb",
            "85e7b4429e3240878f41f9ff503935a9",
            "094d738bf9054054920c44e4d88dd4eb",
            "b424a2cd0d12414298e172d754c715f9",
            "7aee059bbf7147f3b98d394cfffb101a",
            "efb897225439474baf5d8ded3b237e01"
          ]
        },
        "id": "z42bdITx2I3Q",
        "outputId": "eed10e37-4dbb-4ba8-ee2a-c43166c230b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizando test:   0%|          | 0/373 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c121eaa47af41a79147f0796367a3ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Archivo 'resultado_temas.xlsx' guardado con 373 filas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "HZtRxD8k4RMW"
      }
    }
  ]
}